import streamlit as st


header = st.container()

with header:
    st.title('О проекте')
    st.header("Сбор данных")
    st.markdown('Для поиска комментариев используется библиотека '
                '[pyyoutube](https://github.com/sns-sdks/python-youtube)'
                ', требующая зарегистрироваться на [Google Cloud Platform]'
                '(https://console.cloud.google.com/) и получить api-key '
                'проекта для работы с API YouTube. Реализовав парсер комментариев '
                "по URL-видео, в итоге получаем сырые 'замусоренные' текстовые данные, "
                'требующие дальнейшей обработки.')

    st.header("Предобработка текста")
    st.markdown("Для очистки 'замусоренных' комментариев используются некоторые техники NLP. Например:\n"
                "+ удаление смайлов (эмодзи);\n"
                "+ удаление знаков пунктуации, html знаков, ссылок, цифр;\n"
                "+ удаление комментариев без русскоязычных символов;\n"
                "+ токенизация текста на слова (токен=слово);\n"
                "+ лемматизация слов;\n"
                "+ удаление стоп-слов.\n"
                "Далее комментарии фильтруются по минимальному количеству токенов (3), "
                "а токены по минимальному количеству символов (2). В результате получаются "
                "очищенные тексты, готовые для моделирования.")
    st.header("Векторизация текста")
    st.markdown("Для перевода с языка людей на язык машин текст представляется в виде чисел. "
                "Для решения этой задачи используется модель "
                "[FastText](https://radimrehurek.com/gensim/models/fasttext.html), хорошо "
                "зарекомендовавшая себя "
                "для построения эмбеддингов русскоязычных слов, так как под капотом "
                "дробит слова на n-граммы, "
                "что помогает учитывать многообразие словоформ и решает проблему опечаток. "
                "Модель каждый раз заново учится на корпусе очищенных текстов и преобразует "
                "слова в вектор чисел "
                "с размерностью 100. Эмбеддинг комментария создается усреднением суммы "
                "векторов токенов, "
                "входящих в конкретный комментарий. В итоге векторизации получаем матрицу с размерностью "
                "(количество комментариев, 100).")
    st.header("Понижение размерности")
    st.markdown("Визуализировать вектор у которго 100 признаков сложно, к тому же большие "
                "размерности увеличивывают вычислительные затраты. Поэтому размерность понижается до 3 "
                "при помощи алгоритма [UMAP](https://umap-learn.readthedocs.io/en/latest/clustering.html) "
                "с указанием метрики косинуснового расстояния."
                )
    st.header("Кластеризация")
    st.markdown("В проекте используются 5 алгоритмов кластеризации. Алгоритмы делятся на -  "
                "требующие заранее указанного числа кластеров:\n"
                "+ [KMeans](https://scikit-learn.org/stable/modules/generated/"
                "sklearn.cluster.KMeans.html);\n"
                "+ [Spectral Clustering](https://scikit-learn.org/stable/modules/generated/sklearn."
                "cluster.SpectralClustering.html).\n"
                "И алгоритмы сами определяющие число кластеров:\n"
                "+ [Agglomerative Clustering](https://scikit-learn.org/stable/modules/generated/"
                "sklearn.cluster.AgglomerativeClustering.html);\n"
                "+ [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn."
                "cluster.DBSCAN.html);\n"
                "+ [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/).\n"
                "У алгоритмов, которым требуется указать предварительное числа кластеров, "
                "сначала высчитываются 3 метрики: "
                "*Calinski-Harabasz Index*, *Davies-Bouldin Index*, *Silhouette Score* "
                "в диапазоне от 3 до 21 кластера и выбирается такое число кластеров, которое "
                "максимизирует обобщенную нормализованную метрику. В алгоритмах HDBSCAN и DBSCAN "
                "в виду специфики может появится отдельный кластер *Шум*."
                )
    st.caption("Выполнил: Абдракипов Родион")

