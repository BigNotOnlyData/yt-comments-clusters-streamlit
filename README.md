# yt-comments-clusters-streamlit
[Здесь](https://bignotonlydata-yt-comments-clusters-streamlit---0jl6pv.streamlitapp.com/)

Проект по кластеризации комментариев из YouTube. Комментарии извлекаются из видео-ролика, 
переданного по URL, проходят предобработку, анализируются и подаются в модели кластеризации. 
Для каждой модели выводятся результаты в виде трёхмерной визуализации, количественного 
распределения комментариев по кластерам, облака слов.

## Сбор данных
Для поиска комментариев используется библиотека [pyyoutube](https://github.com/sns-sdks/python-youtube)
, требующая зарегистрироваться 
на [Google Cloud Platform](https://console.cloud.google.com/) и получить 
api-key проекта для работы с API YouTube. 
Реализовав парсер комментариев по URL-видео, в итоге получаем сырые 'замусоренные' 
текстовые данные, требующие дальнейшей обработки.

## Предобработка текста
Для очистки 'замусоренных' комментариев используются некоторые техники NLP. Например:
+ удаление смайлов (эмодзи); + удаление знаков пунктуации, html знаков, ссылок, цифр;
+ удаление комментариев без русскоязычных символов;
+ токенизация текста на слова (токен=слово);
+ лемматизация слов;
+ удаление стоп-слов.

Далее комментарии фильтруются по минимальному количеству токенов (3), а токены по 
минимальному количеству символов (2). В результате получаются очищенные тексты, 
готовые для моделирования.

## Векторизация текста
Для перевода с языка людей на язык машин текст представляется в виде чисел. 
Для решения этой задачи используется модель [FastText](https://radimrehurek.com/gensim/models/fasttext.html)
, хорошо зарекомендовавшая 
себя для построения эмбеддингов русскоязычных слов, так как под капотом дробит 
слова на n-граммы, что помогает учитывать многообразие словоформ и решает проблему 
опечаток. Модель каждый раз заново учится на корпусе очищенных текстов и преобразует 
слова в вектор чисел с размерностью 100. Эмбеддинг комментария создается усреднением 
суммы векторов токенов, входящих в конкретный комментарий. В итоге векторизации 
получаем матрицу с размерностью (количество комментариев, 100).

## Понижение размерности
Визуализировать вектор у которго 100 признаков сложно, к тому же большие 
размерности увеличивывают вычислительные затраты. Поэтому размерность 
понижается до 3 при помощи алгоритма [UMAP](https://umap-learn.readthedocs.io/en/latest/clustering.html) 
с указанием метрики косинуснового расстояния.

## Кластеризация
В проекте используются 5 алгоритмов кластеризации. Алгоритмы делятся на - требующие заранее указанного числа кластеров:

+ [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html);
+ [Spectral Clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html).

И алгоритмы сами определяющие число кластеров:

+ [Agglomerative Clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html);
+ [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html);
+ [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/).

У алгоритмов, которым требуется указать предварительное числа кластеров, сначала 
высчитываются 3 метрики: Calinski-Harabasz Index, Davies-Bouldin Index, Silhouette 
Score в диапазоне от 2 до 30 кластера и выбирается такое число кластеров, которое 
максимизирует обобщенную нормализованную метрику. В алгоритмах HDBSCAN и DBSCAN 
в виду специфики может появится отдельный кластер Шум.
